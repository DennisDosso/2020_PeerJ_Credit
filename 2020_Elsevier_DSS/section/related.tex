\section{Background}
\label{sec:related}

\paragraph{Data in Research} %As described by Jim Gray in his last talk~\citep{hey2009jim}, 
The world of research is rapidly transitioning towards the \emph{fourth paradigm of science}~\citep{hey2009jim}, that is, data-intensive scientific discovery, where data are important for scientific advances as well as for traditional publications~\citep{Bechhofer2013linkisnotenough}.

The scientific community is promoting an \emph{open research culture}~\citep{nosek2015promoting}, founded on methods and tools to share, discover, and access experimental data. 
The community has identified the FAIR principles (Findable, Accessible, Interoperable, and Reusable)~\citep{fair2016Wilikinson}, that should be enforced by every database. 
In particular, data should be accessible from the articles, journals, and papers that cite or use them~\citep{cousijn2019bringing}.
Aspects such as the need for the \emph{reproducibility} of experiments through the used data; the \emph{availability} of scientific data; the \emph{connections} between data and the scientific results are all needed aspects for the fourth paradigm, and are all relevant to the domain of \emph{data citation}~\citep{honor2016data}.

\paragraph{Data Citation: Principles and Motivations} Data Citation principles were proposed in \citep{CODATA2013}, and later summarized and endorsed by the Joint Declaration of Data Citation Principles (JDDCP)~\citep{martone2014joint}. 
The principles are divided into two groups~ \citep{Silvello18jasist}. The first one contains principles concerning the role of data citation in scholarly and research activities such as the (i) \emph{importance} of data (why data citation is important and why data should be considered as first-class citizens); (ii) \emph{credit} and \emph{attribution} to the creators and curators of the data; (iii)\emph{evidence}; (iv) \emph{verifiability}; and \emph{interoperability}, with these last three requiring data citation methods to be flexible enough to operate through different communities. 
The second group defines the main guidelines to establish a data citation systems, and contains principles such as the (i) \emph{unique identification} of the data being cited; (ii) \emph{(open) access} to data; (iii) guarantee of \emph{persistence} and \emph{availability} of citations even after the lifespan of the cited entity; the (iv) \emph{specificity} of a citation, i.e. it must lead to the data set originally cited.

It is possible to outline six main motivations for data citation~\citep{Silvello18jasist}:
\begin{itemize}
	\item \emph{Data attribution}: identify the individuals that should be credited for data with variable granularity.
	\item \emph{Data connection}: connect papers to the data being used.
    \item \emph{Data Discovery}: citations helps to find data records and subsets that would be otherwise not findable via search engines.
    \item \emph{Data Sharing}: share data obtained by researchers within the whole community. 
    \item \emph{Data Impact}: highlight the results obtained in writing papers using specific data, the frequency and modality data were used.
    \item \emph{Reproducibility}: data citation greatly impacts the reproducibility of science~\citep{baggerly2010disclose}. Many authoritative journals ask to share data and provide valid methodologies to reproduce experiments.
\end{itemize}

\subsection{Data Citation in Relational Databases}
In this paper, we develop our methods and experiments on relational databases. RDBs have been the main target of data citation methods since the surge of the data-centric research paradigm. 
The RDA ``Working Group on Data Citation: Making Dynamic Data Citable''\footnote{\url{https://www.rd-alliance.org/groups/data-citation-wg.html}}~\citep{RauberEtAl2016} has been working in the last years on large, dynamic, and changing datasets. The working group has finished the development of its guidelines and has now moved on into an adoption phase. 
The datasets considered by the Working Group are often relational.

In one of its most recent sessions~\citep{rauber2015data}, the Working Group (WG) on Data Citation reported that there are various implementations of its guidelines for Data Citation on MySQL/Postgres relational databases. 
Some of these databases are: DEXHELPP\footnote{\url{http://www.dexhelpp.at/}} (Social Security Records); NERC (ARGO Global Array); EODC (Earth Observation Data Centre)~\citep{gosswein2019data}; LNEC (River dam monitoring); MDS (Million Song Database)~\citep{bertin2011million}; CBMI\footnote{\url{https://medicine.missouri.edu/centers-institutes-labs/center-for-biomedical-informatics}} (Center for Biomedical Informatics); VMC (Vermont Monitoring Cooperative); CCA\footnote{\url{https://ccca.ac.at/startseite}} (Climate Change Center Austria); VAMDC (Virtual Atomic and Molecular Data Center)~\citep{Dubernet_2016, ZwolfEtAl2016}.

More examples of work on data citation in relational databases are~\citep{bunemann2016citation, WuSIGMOD18, AlawiniDHW17,davidson2017model}. 
The website \texttt{\url{https://fairsharing.org/}} keeps a long updated list of curated and scientific databases (many of which are relational or graph-based) following FAIR guidelines. These databases are citable since they are compliant with the most recent guidelines, and they are in the vast majority of cases accessible via dynamically created Webpages. 
In all these databases it is, therefore, possible to implement DCD on top of the existing infrastructures for citing data.

Data citation techniques are primarily applied to relational databases because of their diffusion and also because the portions of data that are to be cited are easily identified: the whole database, a relation, a tuple, or even an attribute. 
Many papers \citep{buneman2006cite, bunemann2016citation, AlawiniDHW17} consider more complex citable units, recognizing that often the \emph{views} of a database are the ones to be cited. Generally, a \emph{view} is a query on the database.
To this end, \citep{WuSIGMOD18} suggested decomposing the database in a set of views, where each view is associated with its citation. 


At present, the most common practices to cite databases include:
\begin{enumerate}
    \item A database cited as a whole, even though only parts of the databases are used in the papers or datasets. Alternatively, the so-called ``data papers'' are cited, being traditional papers that describe a database~\citep{CandelaEtAl2015}. \\
    In this case, all the credit from the citations goes to the database administrators or to the authors of the data papers. 
    \item Subsets of data, obtained by issuing queries to a database, are individually cited. This is the solution adopted by the \emph{Resource Data Alliance} (RDA) working group on Data Citation~\citep{RauberEtAl2016}.
    In this case, the credit generated from citations is distributed among the contributors of the portions of data being cited, and/or to the database administrators. 
    \item The database is accessible via a series of Webpages that arrange the content of the database by topic or theme. Examples in the life science domain include the Reactome Pathway database~\citep{reactome2016}, the GtoPdb \citep{iuphar2018}, and the VAMDC~\citep{ZwolfEtAl2016}.
     Every single Webpage is unequivocally identifiable and can be individually cited. 
\end{enumerate}

\eat{
\textcolor{red}{Despite all the research efforts dedicated to the study and promotion of data citation, none of the largest citation-based systems, such as Elsevier Scopus, Web of Science, Microsoft Academia, or Google Scholar, consider scientific datasets as citable objects in academic work. 
Clarivate Analytics Data Citation Index (DCI) \citep{force2016research} is an exception, since its infrastructure tracks data usage in scientific domains and provides the technical means to connect datasets and repositories to scientific papers. However, DCI considers only citations to (previously registered and approved) databases as a whole and does not count citations to database portions such as views, tables, or tuples.}
\scream{GMS: I'd remove (eat env.) this part about citation-based indexes because we are not proposing any solution for that problem.}
}

%Scholix
%Publishers, data centers, and indexing services have started to create bidirectional links between research data and scholarly literature. Such links, however, usually stem from agreements implemented by two organizations. They, therefore, lack a universally accepted industrial standard, and each agreement differs from the other~\citep{burton2017scholix}. 
%The rapid growth of bilateral agreements hinders interoperability, which is one of the principles of data citation. In fact, this kind of agreement has generated a series of undesirable side-effects. Many publishers, data centers, repositories, and infrastructure providers remain disconnected.
%Moreover, the heterogeneity that ensues from (considerably different) agreements and practices hinders the global interoperability among different agreements. 
%One example of such heterogeneity may be found in identification systems such as Digital Object Identifiers\footnote{\url{https://www.doi.org}} (DOI) and Life Science Identifiers\footnote{\url{http://www.lsid.info}} (LSID).
%
%The Scholix framework \citep{burton2017scholix} addresses this issue. As a community and multi-stakeholder driven effort, it strives to facilitate information exchange between data and literature and between data and data. 
%It can be regarded as a framework, a set of guidelines, and lightweight models to facilitate interoperability among link providers. 



\subsection{Data Credit}
Data credit is related to data citation: they both aim to recognize the work of data creators and curators. 
Data credit can therefore also be seen as a by-product of data citation, since credit attribution is impossible without the presence of data citations.

\citet{transitiveCreditKatz2014} suggests the need for a \emph{modified citation system} that includes the idea of \emph{transient} and \emph{fractional credit}, to be used by developers of research products as software and data.
In the paper two considerations are made: (i) research objects such as data and software are currently not formally rewarded or recognized by the community; 
(ii) even in traditional papers, the contribution of each author to the work is hard to understand, unless explicitly specified in the paper. 
This is even more true for data, where different groups of people work on the same database.

In \citep{transitiveCreditKatz2014} credit is defined as a ``quantity'' that describes the importance of a research entity, such as papers, software, or data, mentioned in a citation. 
It also proposed the idea of a \emph{distribution} of credit from research entities, such as papers or data, to other research entities through citations. 
\rone{Therefore, when discussing data credit, we need to consider  \emph{credit computation} -- i.e., the process to compute the quantity of credit generated by the citation -- and \emph{credit distribution} -- i.e., the process to distribute credit and to assign it to the entities that contributed to the creation/curation of the cited data. In this paper we focus on the latter.}

\rone{These two processes are} done by exploiting the structure of the \emph{citation graph}, a directed graph whose nodes are publications and edges are citations.
This graph is the model at the core of systems such as Google Scholar and the Web of Science.
We add to this that the concept of credit can be built on top of the existing infrastructure handling traditional and data citations.

\citet{transitiveCreditKatz2014} further explores the idea of a \emph{distribution} of credit from research entities (i.e., papers and data) to other research entities through citations that connect them. 
Thanks to traditional citations and now also to data citations, this distribution is finally possible, at least between papers and data. 
Some problems related to traditional citations can thus be solved by citations:

\begin{enumerate}
\item Credit rewards research entities that to date are not (formally) recognized (a goal shared with data citation).
\item Credit can reward authors \emph{proportionally} to their role in generating the entity. The more an author contributes to a paper, the more credit is given to him. \citet{ZouP16} work on something similar with their zp-index, which includes in its formulation the position (and thus the role) of a publication author to represent its impact in the work itself.
\item Credit can be \emph{transitively} channeled through a chain of papers citing each other, thus enabling the rewarding of older papers that are no more cited, since other papers summarize or report their content 
%\textbf{Gianmaria: I do not understand this token, what do you mean with: papers that are no more cited?} 
but are nevertheless crucial in a research area for the influence of their content.
\end{enumerate}

\citet{creditFang18} presents a framework to distribute the credit generated by a paper to its authors and to the papers in its reference list in a transitive way. 
Let us consider the \emph{citation graph} as the graph where the nodes are papers and the links are the citations among them.
In this graph, every paper is a source of credit, which is then transferred to the neighboring nodes.
The quantity of credit received by each cited paper depends on its impact/role in the citing paper. 
So far, this theoretical framework is limited to papers, but it can be easily extended to a citation graph including both papers and data. 

\citet{zeng2020assigning} proposes the first method to compute credit within a network of papers citing data. 
Adopting a network flow algorithm, they simulate a random walker to estimate a score for each dataset, leveraging real-world usage data to compute the credit.
This is the first step towards an automatic credit computation procedure.
This proposal is, however, limited to assigning credit to whole datasets, and it does not deal with the granularity of data. It does not work to assign credit to a single research entity within a dataset. 
Differently from \citet{zeng2020assigning}, we do not treat the credit computation process, but we focus on the distribution process.

%%%%%%%%%%%
\subsection{Data Provenance}
\label{section:related_provenance}

To distribute credit, we base our methods on \emph{data provenance}. 
Data provenance is information that describes the origin and the process of creation of data. It can also be seen as metadata pertaining to the derivation history of the data. 
It is particularly useful to help users to understand where data are coming from, and the process they went through. 
Data citation and data provenance are closely linked~\citep{AlawiniDSTW17} since both are forms of annotations on data retrieved through queries. 
Data provenance has been widely studied in different areas of data management. 
In this paper, we focus on provenance for database management systems (DBMS). For further details on data provenance, please refer to surveys like \citep{CheneyProvSurvey} and \citep{SimmhanPG05}.

\citet{CheneyProvSurvey} presents four main types of data citation for DBMS: \emph{lineage}~\citep{lineageCui}, \emph{why-provenance}~\citep{WhyProvBuneman}, \emph{how-provenance}~\citep{howProvenanceGreen} and \emph{where-provenance}~\citep{WhyProvBuneman}.

Let us start with the first three provenances. Given a database instance $I$, a query $Q$, and the result $Q(D)$, consider one tuple $t$ of the output. 
Its provenance is information about its generation through the tuples of the input that are used by $Q$. Different types of provenance convey different levels of information. Since these three provenances are computed for each tuple of the output, they are also referred to as \emph{tuple-based}.

%Lineage is the simplest among the forms of provenance. It has been defined in different ways~\citep{CheneyProvSurvey}, but it can be thought of as the set of all the tuples that are used in some way by the query to produce the output tuple, the ones that are somehow \emph{relevant} to its generation. 
%
%The definition of why-provenance is based on the notion of \emph{witness set}. A witness is a set of relevant tuples that guarantees the existence of $t$ in $Q(D)$. The lineage is therefore an example of a witness. The why-provenance of a tuple $t$ is a peculiar set of witnesses  -- described in \citep{WhyProvBuneman} -- that are computed from the query, called \emph{witness basis}. 
%A witness basis may be composed of more than one witness. 
%Therefore, the why-provenance contains more information than the lineage, since it describes \emph{alternative} ways in which the same output may be generated. 
%
%The how-provenance takes the form of a polynomial, called \emph{provenance polynomial}, where the variables are taken from the set of identifiers of the tuples (provided that each tuple in $I$ has an identifier) and the coefficients are drew from $\mathbb{N}$. 
%This provenance also contains information on \emph{how} the input tuples are used. For example, when two tuples are combined by a join, they are also combined in the polynomial by the $\cdot$ operator. When two or more tuples become equivalent due to a union or a projection, the corresponding monomials are combined by the $+$ operator.
%
%It has been shown in \citep{CheneyProvSurvey} that the how-provenance is the more general and informative of the three, containing the other two.

Where-provenance, differently from the other three, is \emph{attribute-based}, so we do not take it into account in this work since we consider the tuple as the finest citable unit. 

\rtwo{We also consider the notions of causality and responsibility, as defined in 
\cite{MeliouGMS11}. Causality is an enrichment of lineage, and it is the attribution of a certain degree of importance to the tuples of the lineage based on their role in the generation of the output. Responsibility is a value given to the tuples of the lineage to rank them based on their degree of causality (the more important the role of a tuple in generating the output, the higher its responsibility)}.
