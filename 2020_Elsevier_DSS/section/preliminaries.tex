\section{Data Provenances}
\label{section:preliminaries}
We now describe the three types of provenance used in this paper --  lineage, why-provenance, and how-provenance -- as well as
\rtwo{the notion of Causality and Responsibility}. 

\subsection{Lineage}
\rone{Lineage is the simplest form of provenance. It was first introduced by \citet{lineageCui}, and can be thought of as the set of all tuples that
%are \emph{relevant} (or that 
are used by the query to generate the output~\cite{CheneyProvSurvey}.}


As an example, consider the following SQL query \texttt{Q1}, applied to the database described in Table \ref{table:running_example}, asking for the names of families curated by researchers based in the United Kingdom (UK):

\vspace{2mm}
{\footnotesize
\begin{adjustwidth}{25pt}{0pt}
\begin{verbatim}
		Q1: SELECT DISTINCT f.name
		FROM family AS f JOIN contributor2family AS c2f 
		ON f.id = c2f.family_id
		JOIN contributor AS c ON c2f.contributor_id = c.id
		WHERE c.country = 'UK'
\end{verbatim}	
\end{adjustwidth}
}
\vspace{2mm}

\begin{table}[hbt]
\centering
  \begin{tabular}{|l||c|}
  \hline
    id & name\\
    \hline
    $o_1$ &  Dopamine Receptors\\
    $o_2$ & YANK Family\\
    \hline
  \end{tabular}
  \begin{tabular}{c}
  	lineage   \\
  	$\{f_1, c2f_1, c_1, c2f_2, c_2\}$ \\
  	$\{ f_4, c2f_4, c_1\}$ \\
  \end{tabular}
    \caption{Result of \texttt{Q1} over the database instance in Table \ref{table:running_example} with the lineage of each output tuple. %, which asks for the names of families curated by a researcher based in the UK.
    Attribute \texttt{id} is not part of the output, and was added to identify each tuple.}
  \label{table:result}
\end{table}

Table \ref{table:result} shows the query output, which consists of two tuples. We add an extra  attribute \texttt{id} so that we can easily refer to each result tuple.  
The lineage for tuple $o_1$ is the set $\{f_1, c2f_1, c_1, c2f_2, c_2\}$, since the tuple $f_1$ was joined with $c2f_1$ and then with $c_1$, and was also joined with $c2f_2$ and $c_2$. No other tuple is used in the database to produce $o_1$.
For tuple $o_2$ the lineage is $\{ f_4, c2f_4, c_1\}$.
Lineage is defined for each tuple of the output, and can differ between tuples.

\subsection{Why-Provenance}
Why-Provenance was first defined in terms of a deterministic semistructured data model and query language \citep{WhyProvBuneman}.  We use here its definition in terms of the relational model~\citep{CheneyProvSurvey}.

While lineage aims to find all and only the tuples in the input relevant to the production of an output tuple, why-provenance aims to find sub-instances of the input that ``witness'' a part of the output. 
Given a tuple $t$ in the query's output $Q(D)$, a \emph{witness} is any sub-instance of the database that produces $t$, i.e., a set that guarantees the existence of $t$ in $Q(D)$.
In particular, the whole database and the lineage of $t$ are both examples of witnesses of $t$.
Since the definition of witness allows for the presence of ``irrelevant'' tuples, the set of all witnesses is finite (since the database instance $I$ is finite), but it is potentially exponentially large~\citep{CheneyProvSurvey}.

\citet{WhyProvBuneman} defined the why-provenance of an output tuple $t$ in the result $Q(I)$ as a special \emph{subset} of the set of witnesses called the \emph{witness basis}.
%The witnesses of the basis depend on $Q$; thus, each basis's size is bounded by the size of $ Q $. 
The witnesses of the basis exclude tuples that are irrelevant to $t$ being produced by $Q$, and thus the basis tends to be very small compared to the set of all possible witnesses~\citep{CheneyProvSurvey}.
%The witnesses are also {\em minimal}, in the sense that if one tuple is removed from one of these witnesses, it cannot produce the output. 
% This is not true for the lineage. For example, it is sufficient to consider the lineage of $o_1$ in the example above, where the tuples $c2f_2$ and $c_2$ may be eliminated without affecting the output. 


%FORMAL DEFINITION
%\begin{definition}{Witness~\citep{CheneyProvSurvey}}\\
%\label{def:witness}
%	Let $I$ be a database instance, $Q$ a query over $I$, and $t$ a tuple in $Q(I)$. An instance $I' \subseteq I$ is a \emph{witness} for $t$ with respect to $Q$ if $t \in Q(I')$.
%\end{definition}
%In other words, a witness is a set of tuples in the input that guarantees the presence of $t$ in the output. Any sub-instance of the database that produces $t$ is a witness. In particular, the whole database and the lineage of $t$ are both witnesses of $t$. We can define the set of all the witnesses as $Wit(Q, I, t) = \{ J \subseteq I | t \in Q(J) \}$.
%This set is finite (if $I$ is also finite), but it is potentially exponentially large due to the possibility for witnesses to contain ``irrelevant'' tuples. 
%Recalling that $TupleLoc$ is the set of all the tuple locations, we say that the witness basis belongs to $\mathcal{P}(\mathcal{P}(TupleLoc))$, i.e. it is a set of sets of tuples. We note that if $TupleLoc$ is finite, then also $\mathcal{P}(\mathcal{P}(TupleLoc))$ is finite.

%The formal definition of why-provenance in the context of relational databases is as follows:
%
%\begin{definition}{Why-provenance~\citep{CheneyProvSurvey}}\\
%	Let $Q$ be an $SPJRU$ query. Let $I$ be a database instance, and $t$ be a tuple in $Q(I)$. Then, the \emph{why-provenance} (\emph{witness basis}) of $t$ according to $Q$ and $I$, denoted as $Why(Q, I, t)$, is a subset of $\mathcal{P}(\mathcal{P}(TupleLoc))$ defined as follows:
%	\[
%\begin{array}{rl}
%      Why(\{t\}, I, \{u\}) = & \begin{cases}
%		\{ \emptyset \}, & \mbox{if } t = u\\
%		\emptyset, & \mbox{otherwise.}\\
%	\end{cases}\\
%	Why(R, I, t) = & \begin{cases}
%		\{\{(R, t)\}\}, & \mbox{if } t \in R(I),\\
%		\emptyset, & \mbox{otherwise.}\\
%	\end{cases}\\
%	Why(\sigma_{\theta}(Q), I, t) = & \begin{cases}
%		Why(Q, I, t), & \mbox{if } \theta(t),\\
%		\emptyset, & \mbox{otherwise.}
%	\end{cases}\\
%	Why(\pi_U(Q), I, t) = & \bigcup \left \{Why(Q, I, u) | u \in Q(I), t = u[U] \right \} \\
%	Why(\rho_{A \mapsto B}(Q), I, t) = & Why(Q, I, t[B \mapsto A])\\
%	Why(Q_1 \Join Q_2, I, t) = & Why(Q, I, t[U_1]) \Cup Why(Q_2, I, t[U_2]) \\
%	Why(Q_1 \cup Q_2, I, t) = & Why(Q_1, I, t) \cup Why(Q_2, I, t)\\
%\end{array}
%\]
%Where the symbol $\Cup$ takes all the pairwise unions of two collections, working in a similar way to the strict union defined above for lineage. That is: $S \Cup \bot = \bot \Cup S = \bot$, and $S \Cup T = \{s \cup t | s \in S, t \in T\}$ otherwise.
% \end{definition}
% 
% As can be inferred from this definition, the witnesses of the witness basis depend on the syntax of $Q$, thus the size of each witness basis is bounded by the size of $Q$. In particular, the witnesses of the witness basis exclude tuples that are irrelevant to $t$ being produced by $Q$. Thus, the basis tends to be very small when compared to the set of all possible witnesses~\citep{CheneyProvSurvey}.
 

\begin{table}[hbt]
\centering
  \begin{tabular}{|l||c|}
  \hline
    id & name\\
    \hline
    $o_1$ &  Dopamine Receptors\\
    $o_2$ & YANK Family\\
    \hline
  \end{tabular}
  \begin{tabular}{c}
  	why-provenance   \\
  	$\{\{f_1, c2f_1, c_1\}, \{f_1, c2f_2, c_2\}\}$ \\
  	$\{\{ f_4, c2f_4, c_1\}\}$ \\
  \end{tabular}
    \caption{Result of \texttt{Q1} over the database instance in Table \ref{table:running_example} with the why-provenance of each output tuple.}
  \label{table:result_why_prov}
\end{table}
 
In a sense, each witness in the witness basis captures one possible way in which a tuple in the output was generated by the query. 
To better understand this, consider the example in Table \ref{table:result_why_prov}, where each tuple in the result of query \texttt{Q1} is annotated with its why-provenance. 

The why-provenance of output tuple $o_2$ has only one witness, which coincides with its lineage. This happens because there is only one way this output tuple can be produced, i.e., for tuple $f_4$ to be joined with $c2f_4$ and $c_1$.
On the other hand, $o_1$ has a witness basis of two witnesses, since there are two possible ways in which the query can generate $o_1$. 
One possibility is that $f_1$ is joined with $c2f_1$ and $c_1$ (the first witness), and the second possibility is that $f_1$ is joined with $c2f_2$ and $c_2$ (the second witness). This means that to generate $o_1$, it is sufficient that only one of the two witnesses is present in the input database. 

\subsection{How-Provenance}
\label{section:how_provenance_tuples}

While why-provenance describes the source tuples that witness an output tuple in the result of the query, it leaves out  information about how the source tuples are used.
How-provenance was therefore defined in \citep{howProvenanceGreen} to capture this information using a \emph{semiring} algebraic structure.
It takes the form of a polynomial, called \emph{provenance polynomial}, where the variables are taken from the set $X$ of identifiers of the tuples (provided that each tuple in $I$ has an identifier) and the coefficients are drew from the set of natural numbers $\mathbb{N}$.\footnote{\rtwo{This semiring is commonly referred as $\mathbb{N}[X]$ in the literature.}}
%\scream{Why do we need this last? -- I added it because reviewer 2 talked about PosBool(X), a semiring contained in N[X]. I wanted to stree the fact that we work here, not in PosBool.}
% SOME FORMALISM
%A semiring $K$ is a \emph{set} equipped with two operations, typically denoted with the symbols $+$ and $\cdot$, satisfying the following axioms ~\cite[pg. 26]{berstel1985theory}:
%\begin{enumerate}
%	\item The set $K$ is a \emph{commutative monoid} for the operator $+$ with a neutral element $0$. Therefore, it has these properties:
%		\begin{enumerate}
%			\item $(a + b) + c = a + (b + c)$ (associative property)
%			\item $0 + a = a + 0 = a$ ($0$ is the neutral element) 
%			\item $a + b = b + a$ (commutative property)
%		\end{enumerate}
%	\item The set $K$ is a \emph{monoid} with identity element $1$. Therefore, it has these properties:
%		\begin{enumerate}
%			\item $(a \cdot b) \cdot c = a \cdot (b \cdot c)$ (associative property)
%			\item $1 \cdot a = a \cdot 1 = a$ ($1$ is the neutral element)
%		\end{enumerate}
%	\item Multiplication is distributive on addition, i.e.:
%		\begin{enumerate}
%			\item $a \cdot (b + c) = (a \cdot b) + (a \cdot c)$
%			\item $(a + b) \cdot c = (a \cdot c) + (b \cdot c)$
%		\end{enumerate}
%	\item Multiplication by $0$ annihilates $K$, i.e. $\forall x \in K, 0 \cdot x = x \cdot 0 = 0$
%\end{enumerate}

The key idea in \citet{howProvenanceGreen} is to use the two operators $+$ and $\cdot$ to represent two basic transformations that source tuples undergo as a result of applying a relational query to a database~\citep{CheneyProvSurvey}. 
Two tuples may either be joined together (a join is represented with the $\cdot$ operator) or merged via union or projection (represented with the $+$ operator).

 

\begin{table}[]
\centering
  \begin{tabular}{|l||c|}
  \hline
    id & name\\
    \hline
    $o_1$ &  Dopamine Receptors\\
    $o_2$ & YANK Family\\
    \hline
  \end{tabular}
  \begin{tabular}{c}
  	how-provenance   \\
  	$f_1 \cdot c2f_1 \cdot c_1 + f_1 \cdot c2f_2 \cdot c_2$ \\
  	$f_4 \cdot c2f_4 \cdot c_1$ \\
  \end{tabular}
    \caption{Result of \texttt{Q1} over the database instance in Table \ref{table:running_example} with the  how-provenance polynomial of each output tuple.}
  \label{table:result_how_prov}
\end{table} 

Table \ref{table:result_how_prov} shows the two output tuples of our running example annotated with their respective how-provenances. 
Tuple $o_2$ was produced by a join of the input tuples $f_4, c2f_4$, and $c_1$. The three provenance tokens are therefore  ``multiplied'' together. 
The case of $o_1$ is slightly more complex, as already discussed.
It can be obtained by the joins of two different sets of tuples, so there are two monomials combined by $+$ representing these alternative derivations. Each monomial corresponds, in a way, to the witnesses of the why-provenance of $o_1$.
%The $+$ operator represents the fact that the two monomials describe alternative derivations. 
%\scream{Is the reset of this paragraph (in particular the next sentence) necessary or can it be deleted? -- Reviewer 2 seemed to want us to work with set semantics and ditch polynomials. This paragraph should help them clarify the fact that polynomials are actually important and make a difference.-- DD: paragraph commented}
% The output tuple is the result of a merge of two distinct tuples after the projection on the attribute \texttt{name}. This merge is due to the fact that the result of a relational algebra expression is always a {\em set} of tuples, which corresponds to the presence of the \texttt{DISTINCT} operator in an SQL query. 
% This simple example gives the basic idea behind how-provenance and how it allows us to track the operations that produced an output tuple. 

% talking a little bit about coefficients and exponents
Provenance polynomials may also have monomials whose exponents and/or coefficients are greater than one, for example, $3f_1 \cdot c2f_1 \cdot c_1 + f_1 \cdot c2f_2^3 \cdot c_2^3$. This is a polynomial of a tuple produced by a query where the result of the join between the tuples $f_1$, $c2f_1$, and $c_1$ is produced three times and then merged (e.g. as the result of a union), and the tuples $c2f_2$ and $c_2$ are used three times in the operation described by the second monomial (e.g., with nested queries). 
% \scream{Why would the join tuple be produced 3 times?  Perhaps as a result of a union? Projection doesn't make sense}

\rtwo{\subsection{Causality and Responsibility}}
\label{sec:responsibility}

A formal study of causality was introduced in \cite{Halpern2013Causality,ChocklerH04} and later expanded by \citet{MeliouGMS11} to explain the causes of answers and non-answers to queries. 
%Causality is related to the provenance of a query result such as lineage and adds more information to it.
In the following, we refer to the definition of causality and responsibility provided in \cite{MeliouGMS11}. 
In particular, we only focus on answers to a query since non-answers are not relevant in our context.

% paragraph added to make the subsection less formality heavy

There are two types of ``cause" tuples: counterfactual and actual. 
Let $t$ be a tuple in the query's output $Q(I)$, and $t'$ a tuple in its lineage. We call $t'$ a \emph{counterfactual cause} if, by removing $t'$ from $I$, $t$ is also removed from the output (i.e., $t'$ is essential for the generation of $t$). 
We call $t'$ an \emph{actual cause} if there is a set of tuples $\Gamma \subseteq I$ called a \emph{contingency set}, such that $t'$ is a counterfactual cause in $I - \Gamma$. In other words, $t'$ is an actual cause if, even when removed from $I$, there is another set of tuples of the lineage that guarantees the presence of $t$.

\eat{
Let 
%$R_1, \dots, R_k$ be the relation names of a standard relational schema, 
 $D$ be a database instance and $q$ a conjunctive query, let $D^n \subseteq D$ be the set of \emph{endogenous tuples}, i.e. the tuples being actually considered to be possible causes of a query output; while $D^x = D - D^n$ is the set of \emph{exogenous tuples}, the tuples being considered external, unconcerned factors, thus deemed not to be possible causes. 
This distinction between endogenous and exogenous tuple is application dependent, and it can be done by the user at query time. 
%One example is with probabilistic databases with uncertain tuples, where erroneous data may be contained. By considering these uncertain tuples as part of the exogenous tuples dataset, we are factoring them out of the computation of causality. 

% trying to make the theoretical part more "light"
% Then, let $t$ be a tuple in $D^n$ and $o$ be a tuple in the output of query $Q$ on $D$. Then, $t$ is called \emph{counterfactual cause} if, by removing it from the database, we also remove $\bar{a}$ from the answer. In other words, a counterfactual cause is a tuple of the lineage that is fundamental for the presence of $\bar{a}$ in the answer.





Given a tuple $\bar{a}$ with the same arity as the query's answer, we write $D \vDash q(\bar{a})$ when $\bar{a}$ is an answer to $q$ on $D$ and write $D \nvDash q(\bar{a})$ when $\bar{a}$ is a non-answer to $q$ on $D$. 
Causality is defined as follows:

\begin{definition}{Causality \cite{MeliouGMS11} }\\
	Let $t \in D^n$ be an endogenous tuple and $\bar{a}$ a possible answer for $q$. Then:
	\begin{enumerate}
		\item $t$ is called a \emph{counterfactual cause} for $\bar{a}$ in $D$ if $D \vDash q(\bar{a})$ and $D - \{t\} \nvDash q(\bar{a})$
		\item $t \in D$ is called an \emph{actual cause} for $\bar{a}$ if there exists a set $\Gamma \subseteq D^n$, called \emph{contingency} for $t$, such that $t$ is a counterfactual cause for $\bar{a}$ in $D - \Gamma$.
	\end{enumerate}
\end{definition}

$t$ is a \emph{counterfactual cause} if, by removing it from the database, we remove $\bar{a}$ from the answer. Therefore, it can be thought as a tuple of the lineage which is fundamental for the presence of $\bar{a}$ in the answer.
Vice-versa, $t$ is an actual cause if it is possible to find a contingency set of tuples such that, if that set is removed, only then $t$ becomes counterfactual. In other words, when $t$ is an actual cause, even if it was removed from the database, $\bar{a}$ would still be present in the result set thanks to the contingency set. }

Computing the causality of tuples is NP-complete for general queries~\cite{EiterL02}, but for conjunctive queries can be computed in PTIME~\citet{MeliouGMS11}. 


The notion of \emph{responsibility}  measures the degree of causality as a function of the size of the smallest contingency set~\cite{ChocklerH04}. This  allows us to rank lineage tuples based on their degree of causality in generating the output. 

\begin{definition}{Responsibility \cite{MeliouGMS11}}\\
\label{def:responsibility}
	Let $\bar{a}$ be an answer to a query $q$, and let $t$ be a cause. The \emph{responsibility} of $t$ for the answer $\bar{a}$ is:
	\[
		\rho_t = \frac{1}{1 + min_\Gamma|\Gamma|}
	\]
	where $\Gamma$ ranges over all contingency sets for $t$.
\end{definition}

Note that a counterfactual cause will have the maximum responsibility of $1$, and that the larger the minimum contingency of an actual cause is, the smaller its responsibility will be since there are alternatives to  guarantee the presence of the answer $\bar{a}$.


\begin{table}[]
\footnotesize
\centering
  \begin{tabular}{|l|c|}
  \hline
    id & name\\
    \hline
    $o_1$ &  Dopamine Receptors\\
    $o_2$ & YANK Family\\
    \hline
  \end{tabular}
  \begin{tabular}{c}
  	responsibility   \\
  	$f_1=1, c2f_1=0.5, c2f_2=0.5, c_1=0.5, c_2=0.5$ \\
  	$f_4=1, c2f_4=1, c_1=1$ \\
  \end{tabular}
    \caption{Result of \texttt{Q1} over the database instance in Table \ref{table:running_example} with the responsibilities of lineage tuples.}
  \label{table:result_responsibility}
\end{table} 

As an example, consider Table \ref{table:result_how_prov}, where we reported the result set of \texttt{Q1} and the tuples of the lineages with their responsibility values. 
Focusing on $o_1$: the lineage tuple $f_1$ is a counterfactual cause, since its contingency set is empty (when removed from the database, $o_1$ disappears from the result set). Consequently, its responsibility is 1. All the other tuples of the lineage are actual causes. $c_1$, for example, has as minimal contingency set $\{c2f_2\}$ , thus its responsibility is $0.5$. 
For the output tuple $o_2$, all the tuples of the lineage are counterfactual causes, thus their responsibility is 1.

While computing responsibility for general queries is hard~\cite{ChocklerH04}, \citet{MeliouGMS11} proved a dichotomy result for conjunctive queries: for each query without self-joins, either its responsibility can be computed in PTIME in the size of the database or checking if it has a responsibility below a given value is NP-hard.

\rtwo{\subsection{Shapley Value}}
\label{sec:shapley_value}

The Shapley value is named after Lloyd Shapley, who introduced it for the first time in his 1952 work~\cite{Shapley1954}. He considered a \emph{cooperative fame} played by a set $A$ of players, defined by a \emph{wealth function} $v$ that assigns to each coalition set $B \subseteq A$ the wealth $v(B)$.
The question behind the Shapley Value is how to quantify the contribution of each player to the overall wealth. 
Informally, the Shapley value is defined as follows~\cite{LivshitsBKS20}: assume that we select players randomly one by one and without replacement, starting with the empty set. Every time a player $a$ is selected, its addition to the coalition $B$ produces a change in the wealth of the coalition from $v(B)$ to $v(B \cup \{a\})$. The Shapley value of $a$ is the expectation of change that $a$ causes in this probabilistic process.

The Shapley value can be used in different research areas beyond cooperative games, such as economics, law, environmental science, and network analysis. Here, as recently done by \citet{LivshitsBKS20}, we use it as a way of quantifying the contribution of input facts (tuples) to query answers. 
As defined in \cite{Deutch2021Shapley}, in the context of relational databases, given a query $q(\bar{x})$, a database $D$, an input fact $f \in D$ (here seen as a player) and a tuple $\bar{t}$ of same arity as $\bar{x}$, the Shapley value of $f$ in $D$ intuitively represents the contribution of $f$ to the presence (or absence) of $\bar{t}$ in the query result.
Formally, the Shapley value is defined as follows:

\begin{definition}{Shapley value \cite{Deutch2021Shapley}}\\
	Let the database D be partitioned into two sets of facts: a set $D^x$ of  exogenous facts, and a set $D^n$ of endogenous facts. Let q be a Boolean query and $f \in D^n$ be an endogenous fact. The Shapley value of $f$ in $D$ for query $q$ is defined as:
	\begin{multline*}
		Shapley(q, D^n, D^x, f) = \\ \sum_{B \subseteq D^n\backslash \{ f \}}\frac{|B|!(|D^n| - |B| - 1)!}{|D^n|!}  \left(q(D^x \cup B \cup \{ f\} ) - q(D^x \cup B)\right)
	\end{multline*}
\end{definition}

The sum is performed on all possible coalitions of fact $B$ that do not contain the player $f$. Thus, the value $\left(q(D^x \cup B \cup \{ f\} ) - q(D_x \cup B)\right)$ is the wealth brought by $f$ when added to $B$. As we see, the Boolean query is used as wealth function $v$, thus this value will be 1 only when the set $D^x \cup B \cup \{ f\}$ makes the query true, and the set $D_x \cup B$ still makes it false, i.e., when the addition of the fact $f$ is determinant to make the Boolean query true. 
The value $|B|!(|D^n| - |B| - 1)!$ is the number of all the possible permutations over $D^n$ where the facts in $B$ come first, then $f$ is added, and then all the remaining facts. Thus, the value $\frac{|B|!(|D^n| - |B| - 1)!}{|D^n|!}$ can be thought as a weight for the wealth brought by the addition of $f$ to the coalition $B$. 

To extend this definition to non-Boolean queries, we use the same straightforward approach used in \citet{Deutch2021Shapley}: the Shapley value of the fact $f$ for the answer $\bar{t}$ to $q(\bar{x})$ is the value $Shapley(q[\bar{x} / \bar{t}], D^n, D^x, f)$, where $q[\bar{x} / \bar{t}]$ is the Boolean query defined by $q[\bar{x} / \bar{t}](D) = 1$ if and only if $\bar{t}$ is in the output of $q(\bar{x})$ on $D$, and $0$ otherwise.
\scream{DD: I added this paragraph down here hoping to make things clearer. If you think it fails to do so, feel free to delete it.}
In other words, the definition of $Shapley(q, D^n, D^x, f)$ is extended to such queries $q(\bar{x})$ with free variables by considering the query $q[\bar{x} / \bar{t}]$ instead as value function. This query can be seen as a function that takes as input a set of facts and returns $1$ if this set is a witness for $\bar{t}$, and $0$ otherwise.

\begin{table}[]
\footnotesize
\centering
  \begin{tabular}{|l|c|}
  \hline
    id & name\\
    \hline
    $o_1$ &  Dopamine Receptors\\
    $o_2$ & YANK Family\\
    \hline
  \end{tabular}
  \begin{tabular}{c}
  	responsibility   \\
  	$f_1=\frac{7}{15}, c2f_1=\frac{2}{15}, c2f_2=\frac{2}{15}, c_1=\frac{2}{15}, c_2=\frac{2}{15}$ \\
  	$f_4=\frac{1}{3}, c2f_4=\frac{1}{3}, c_1=\frac{1}{3}$ \\
  \end{tabular}
    \caption{Result of \texttt{Q1} over the database instance in Table \ref{table:running_example} with the Shapley values of the tuples of the lineage. In this case $D^n$ corresponds to the lineage.}
  \label{table:result_shapley}
\end{table} 

As an example, consider table \ref{table:result_shapley}, that shows the Shapley values for the lineage's tuples of $o_1$ and $o_2$. In fact, it is only necessary to consider as endogenous tuples the ones of the lineage, since they are the only ones contributing to the generation of the output. In this case, to compute the Shapley value of an input tuple $f$ it is sufficient to compute and the values $\frac{|B|!(|D^n| - |B| - 1)!}{|D^n|!}$ for all the possible sets $B$ such that $B \cup \{f\}$ is a witness and $B$ alone instead is not. 
Thus, suppose we want to compute the Shapley value of the tuple $f_1$. Let us call $\bar{Q}_{1, o_1}$ the Boolean query such that $\bar{Q}_{1, o_1}(D) = 1$ if and only if $o_1$ is in the output of \texttt{Q1}, and $L_{o_1}$ the lineage of $o_1$.
The computation is the following:

\[
\begin{array}{ll}
	Shapley(\bar{Q}_{1, o_1}, L, D - L, f_1) & = \frac{2!2!}{5!} + \frac{2!2!}{5!} + \frac{3!}{5!} + \frac{3!}{5!}  + \frac{3!}{5!} + \frac{3!}{5!} + \frac{4!}{5!}\\
	& = \frac{7}{15}
\end{array}
\]

Where, for the first element of the sum the corresponding $B$ is $\{c2f_1, c_1\}$, for the second element it is $\{c2f_2, c_2\}$, for the third it is $\{c2f_1, c2f_2, c_1\}$, for the fourth it is $\{c2f_1, c_1, c_2\}$, for the fifth it is $\{c2f_2, c_2, c_1\}$, for the sixth it is $\{c2f_1, c2f_2, c_2\}$, and for the seventh $\{c2f_1, c2f_2, c_1, c_2\}$. Every other possible coalition $B$ would make the factor equal to $0$.

Similarly, for tuple $c_1$ (and the other tuples of the lineage), the computation is:
\[
\begin{array}{ll}
	Shapley(\bar{Q}_{1, o_1}, L, D - L, c_1) & = \frac{2!2!}{5!} + \frac{3!}{5!} + \frac{3!}{5!}\\
	& = \frac{2}{15}
\end{array}
\]

As we can see, the sum of the Shapley values of all the tuples in an output tuple's lineage is always equal to 1 when using a Boolean query as wealth function. 



%MORE FORMALISM
%Now we formally introduce the mathematical framework behind how-provenance~\citep{howProvenanceGreen}.
%Let $K$ be a set containing an element $0$. 
%A $K$-$relation$ is a function $R: U$-$Tuples \mapsto K$ which maps every $U$-tuple in an element in $K$ such that its support, defined as $supp(R) = \{t | R(t) \neq 0\}$, is finite. 
%We remember that a $U$-tuple is a tuple with attributes in the set $U$. The $K$-relation is a finite function which models a relation $R$, tagging each tuple in $R$ with an element of $K$ and each tuple that is not in $R$ with $0$.

%\begin{definition}{Operations on the algebraic structure $(K, 0, 1, +, \cdot)$\citep{howProvenanceGreen}}\\
%\label{definition:how_original}
%	Let $(K, 0, 1, +, \cdot)$ be an algebraic structure with two binary operations $+$ and $\cdot$ and two distinguished elements $0$ and $1$. The operations of the positive $K$-relational algebra are defined as follows:
%	\begin{enumerate}
%		\item \textsf{Empty relation}. For any set of attributes $U$, $\exists \emptyset: U-Tuples \mapsto K | \emptyset(t) = 0$.
%		\item \textsf{Selection} Let $R: U$-Tuples $\mapsto K$ and $\sigma$ be a selection predicate that maps each $U$-Tuple to either 0 or 1. Then $\sigma_\theta(R): U$-Tuples $\mapsto K$ is defined by $(\sigma_\theta(R))(t) = R(t) \cdot \sigma(t)$. 
%		\item \textsf{Projection} Let $R: U$-Tuples $\mapsto K$ and $V \subseteq U$. Then $\pi_V(R): V$-Tuples $\mapsto K$ is defined by $(\pi_V(R))(t) = \sum_{t = t'[V] \vee R(t') \neq 0 }R(t')$.
%		\item \textsf{Union} Let $R_1, R_2: U$-Tuples $\mapsto K$. Then $R_1 \cup R_2: U$-Tuples $\mapsto K$ is defined by $(R_1 \cup R_2)(t) = R_1(t) + R_2(t)$.
%		\item \textsf{Natural join} Let $R_1: U_1$-Tuples $\mapsto K$ and $R_2: U_2$-Tuples $\mapsto K$. Then $R_1 \Join R_2: U_1 \cup U_2$-Tuples $\mapsto K$ is defined by $(R_1 \Join R_2)(t) = R_1(t_1) \cdot R_2(t_2)$, where $t_1 = t[U_1]$ and $t_2 = t[U_2]$.
%	\end{enumerate}
%\end{definition}

%It is observed in \citep{howProvenanceGreen, CheneyProvSurvey} that if the $K$-relational semantics satisfies the same equivalence laws as positive relational algebra operators over bags, i.e. union (+) is associative, commutative and has identity $\emptyset$ and join ($\cdot$) is associative, commutative and distributive over union, and projection and selection commute with each other, as well as with union and join, then $(K, 0, 1, +, \cdot)$ must be a commutative semiring. 
% 
%The semiring operations document \emph{how} each output tuple is produced from source tuples. If each source tuple in a database $D$ is tagged with a distinct id, the semiring gives the how-provenance for each output tuple in the form of a polynomial with coefficient from the set $\mathbb{N}$ of natural numbers and variables from the set of source tuples id. 
%
%For clarity, taking inspiration from the original definition \ref{definition:how_original}, we re-define this definition in a compositional manner, as done with lineage and why-provenance. For this, let us consider the algebraic structure $(\mathbb{N}(TupleLoc), 0, 1, +, \cdot)$, where $\mathbb{N}(TupleLoc)$ is the set of polynomials whose coefficients are the natural numbers and the variable are from the set $TupleLoc$. 
%The how-provenance of an output tuple is a function $\mathcal{H} = How(Q, I, o)$ that returns a polynomial in $\mathbb{N}(TupleLoc)$ called \emph{provenance polynomial}. 
%
%
%\begin{definition}{How-Provenance}\\
%\label{definition:how_provenance}
%		Let $Q$ be a (complex) SPJRU query. Let $I$ be a database instance, and $t$ be a tuple in $Q(I)$. Then, the \emph{how-provenance} of ~$t$ according to $Q$ and $I$, denoted as $How(Q, I, t)$, is an element of the set $\mathbb{N}(TupleLoc)$ defined as follows:
%\[
%	\begin{array}{rl}
%		How(\{u\}, I, t) = & \begin{cases}
%			1, & \mbox{if } t = u,\\
%			0 & \mbox{otherwise}.
%		\end{cases}\\
%		How(R, I, t) = & \begin{cases}
%			(R, t), & \mbox{if } t \in R,\\
%			0 & \mbox{otherwise}.
%		\end{cases}\\
%		How(\sigma_\theta(Q), I, t) = & \theta(t) \cdot How(Q, I, t) \\
%		How(\rho_{A \mapsto B}(Q), I, t) = & How(Q, I, t[B \mapsto A]) \\
%		How(\pi_V(Q), I, t) = & \sum_{u \in supp(Q), u[V] = t} How(Q, I, t) \\
%		How(Q_1 \Join Q_2, I, t) = & How(Q_1, I, t[U_1]) \cdot How(Q_2, I, t[U_2]) \\
%		How(Q_1 \cup Q_2, I, t) = & How(Q_1, I, t) + How(Q_2, I, t)\\
%	\end{array}
%\]
%\end{definition}
%
%We remember that $\{u\}$ is a query expression describing a constant, singleton relation, not a relation value per se. These constants correspond to $K$-relations that assign $1$ to $u$ and $0$ to all other tuples.
%The summation in the projection case is finite since the support of a $K$-relation is assumed to be finite. 
%In the selection rule, $\theta$ is seen as a function $\theta: U$-Tuples $\mapsto \{0, 1\}$.